{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the Circuit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\Users\\Saffat\\Downloads\\Circuits_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the years for which data is needed\n",
    "years = [2023, 2024]\n",
    "\n",
    "# Initialize an empty list to store circuit data\n",
    "all_circuits = []\n",
    "\n",
    "# Fetch circuit data for each year\n",
    "for year in years:\n",
    "    url = f\"http://ergast.com/api/f1/{year}/circuits.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        circuits = data.get(\"MRData\", {}).get(\"CircuitTable\", {}).get(\"Circuits\", [])\n",
    "        for circuit in circuits:\n",
    "            circuit_info = {\n",
    "                \"Year\": year,\n",
    "                \"Circuit ID\": circuit.get(\"circuitId\"),\n",
    "                \"Circuit Name\": circuit.get(\"circuitName\"),\n",
    "                \"URL\": circuit.get(\"url\"),\n",
    "                \"Latitude\": circuit.get(\"location\", {}).get(\"lat\"),\n",
    "                \"Longitude\": circuit.get(\"location\", {}).get(\"long\"),\n",
    "                \"Locality\": circuit.get(\"location\", {}).get(\"locality\"),\n",
    "                \"Country\": circuit.get(\"location\", {}).get(\"country\"),\n",
    "            }\n",
    "            all_circuits.append(circuit_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for year {year}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame from the list of circuits\n",
    "circuit_df = pd.DataFrame(all_circuits)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "file_path = r\"C:\\Users\\Saffat\\Downloads\\Circuits_2023_2024.csv\"\n",
    "circuit_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Driver Standings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\\\Users\\\\Saffat\\\\Downloads\\\\Driver_Standings_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the years for which data is needed\n",
    "years = [2023, 2024]\n",
    "\n",
    "# Initialize an empty list to store standings data\n",
    "all_standings = []\n",
    "\n",
    "# Fetch standings data for each year\n",
    "for year in years:\n",
    "    url = f\"http://ergast.com/api/f1/{year}/driverStandings.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        standings_lists = data.get(\"MRData\", {}).get(\"StandingsTable\", {}).get(\"StandingsLists\", [])\n",
    "        for standings in standings_lists:\n",
    "            season = standings.get(\"season\")\n",
    "            round_number = standings.get(\"round\")\n",
    "            driver_standings = standings.get(\"DriverStandings\", [])\n",
    "            for driver in driver_standings:\n",
    "                driver_info = driver.get(\"Driver\", {})\n",
    "                constructor_info = driver.get(\"Constructors\", [{}])[0]\n",
    "\n",
    "                standings_data = {\n",
    "                    \"Year\": season,\n",
    "                    \"Round\": round_number,\n",
    "                    \"Position\": driver.get(\"position\"),\n",
    "                    \"Position Text\": driver.get(\"positionText\"),\n",
    "                    \"Points\": driver.get(\"points\"),\n",
    "                    \"Wins\": driver.get(\"wins\"),\n",
    "                    \"Driver ID\": driver_info.get(\"driverId\"),\n",
    "                    \"Driver Name\": f\"{driver_info.get('givenName')} {driver_info.get('familyName')}\",\n",
    "                    \"Driver Nationality\": driver_info.get(\"nationality\"),\n",
    "                    \"Constructor Name\": constructor_info.get(\"name\"),\n",
    "                    \"Constructor Nationality\": constructor_info.get(\"nationality\"),\n",
    "                }\n",
    "                all_standings.append(standings_data)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for year {year}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame from the list of standings\n",
    "driver_standings_df = pd.DataFrame(all_standings)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "file_path = r\"C:\\\\Users\\\\Saffat\\\\Downloads\\\\Driver_Standings_2023_2024.csv\"\n",
    "driver_standings_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Qualifying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\\\Users\\\\Saffat\\\\Downloads\\\\Qualifying_Results_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the years and rounds for which data is needed\n",
    "years = [2023, 2024]\n",
    "rounds = range(1, 25) \n",
    "\n",
    "# Initialize an empty list to store qualifying results data\n",
    "all_qualifying_results = []\n",
    "\n",
    "# Fetch qualifying results for each year and round\n",
    "for year in years:\n",
    "    for rnd in rounds:\n",
    "        url = f\"http://ergast.com/api/f1/{year}/{rnd}/qualifying.json\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            races = data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "            for race in races:\n",
    "                race_name = race.get(\"raceName\")\n",
    "                circuit_name = race.get(\"Circuit\", {}).get(\"circuitName\")\n",
    "                race_date = race.get(\"date\")\n",
    "                qualifying_results = race.get(\"QualifyingResults\", [])\n",
    "                for result in qualifying_results:\n",
    "                    driver_info = result.get(\"Driver\", {})\n",
    "                    constructor_info = result.get(\"Constructor\", {})\n",
    "\n",
    "                    qualifying_data = {\n",
    "                        \"Year\": year,\n",
    "                        \"Round\": rnd,\n",
    "                        \"Race Name\": race_name,\n",
    "                        \"Circuit Name\": circuit_name,\n",
    "                        \"Date\": race_date,\n",
    "                        \"Position\": result.get(\"position\"),\n",
    "                        \"Driver Name\": f\"{driver_info.get('givenName')} {driver_info.get('familyName')}\",\n",
    "                        \"Driver Nationality\": driver_info.get(\"nationality\"),\n",
    "                        \"Constructor Name\": constructor_info.get(\"name\"),\n",
    "                        \"Constructor Nationality\": constructor_info.get(\"nationality\"),\n",
    "                        \"Q1 Time\": result.get(\"Q1\"),\n",
    "                        \"Q2 Time\": result.get(\"Q2\"),\n",
    "                        \"Q3 Time\": result.get(\"Q3\"),\n",
    "                    }\n",
    "                    all_qualifying_results.append(qualifying_data)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for year {year}, round {rnd}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame from the list of qualifying results\n",
    "qualifying_results_df = pd.DataFrame(all_qualifying_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "file_path = r\"C:\\\\Users\\\\Saffat\\\\Downloads\\\\Qualifying_Results_2023_2024.csv\"\n",
    "qualifying_results_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to C:\\\\Users\\\\Saffat\\\\Downloads\\\\Race_Results_2023_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the years and rounds for which data is needed\n",
    "years = [2023, 2024]\n",
    "rounds = range(1, 23)  # Assuming there are 22 rounds in a season (adjust as needed)\n",
    "\n",
    "# Initialize an empty list to store race results data\n",
    "all_race_results = []\n",
    "\n",
    "# Fetch race results for each year and round\n",
    "for year in years:\n",
    "    for rnd in rounds:\n",
    "        url = f\"http://ergast.com/api/f1/{year}/{rnd}/results.json\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            races = data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "            for race in races:\n",
    "                race_name = race.get(\"raceName\")\n",
    "                circuit_name = race.get(\"Circuit\", {}).get(\"circuitName\")\n",
    "                race_date = race.get(\"date\")\n",
    "                race_results = race.get(\"Results\", [])\n",
    "                for result in race_results:\n",
    "                    driver_info = result.get(\"Driver\", {})\n",
    "                    constructor_info = result.get(\"Constructor\", {})\n",
    "                    fastest_lap = result.get(\"FastestLap\", {})\n",
    "\n",
    "                    race_data = {\n",
    "                        \"Year\": year,\n",
    "                        \"Round\": rnd,\n",
    "                        \"Race Name\": race_name,\n",
    "                        \"Circuit Name\": circuit_name,\n",
    "                        \"Date\": race_date,\n",
    "                        \"Position\": result.get(\"position\"),\n",
    "                        \"Position Text\": result.get(\"positionText\"),\n",
    "                        \"Points\": result.get(\"points\"),\n",
    "                        \"Driver Name\": f\"{driver_info.get('givenName')} {driver_info.get('familyName')}\",\n",
    "                        \"Driver Nationality\": driver_info.get(\"nationality\"),\n",
    "                        \"Constructor Name\": constructor_info.get(\"name\"),\n",
    "                        \"Constructor Nationality\": constructor_info.get(\"nationality\"),\n",
    "                        \"Grid Position\": result.get(\"grid\"),\n",
    "                        \"Laps Completed\": result.get(\"laps\"),\n",
    "                        \"Status\": result.get(\"status\"),\n",
    "                        \"Race Time\": result.get(\"Time\", {}).get(\"time\"),\n",
    "                        \"Fastest Lap Rank\": fastest_lap.get(\"rank\"),\n",
    "                        \"Fastest Lap Time\": fastest_lap.get(\"Time\", {}).get(\"time\"),\n",
    "                        \"Fastest Lap Average Speed\": fastest_lap.get(\"AverageSpeed\", {}).get(\"speed\"),\n",
    "                    }\n",
    "                    all_race_results.append(race_data)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for year {year}, round {rnd}. HTTP Status Code: {response.status_code}\")\n",
    "\n",
    "# Create a DataFrame from the list of race results\n",
    "race_results_df = pd.DataFrame(all_race_results)\n",
    "\n",
    "# Save the DataFrame to a CSV file in the specified folder\n",
    "file_path = r\"C:\\\\Users\\\\Saffat\\\\Downloads\\\\Race_Results_2023_2024.csv\"\n",
    "race_results_df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Images 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.formula1.com/en/racing/2024/qatar\n",
      "Processing https://www.formula1.com/en/racing/2024/australia\n",
      "Processing https://www.formula1.com/en/racing/2024/china\n",
      "Processing https://www.formula1.com/en/racing/2024/mexico\n",
      "Processing https://www.formula1.com/en/racing/2024/emiliaromagna\n",
      "Processing https://www.formula1.com/en/racing/2024/azerbaijan\n",
      "Processing https://www.formula1.com/en/racing/2024/las-vegas\n",
      "Processing https://www.formula1.com/en/racing/2024/singapore\n",
      "Processing https://www.formula1.com/en/racing/2024/netherlands\n",
      "Processing https://www.formula1.com/en/racing/2024/pre-season-testing\n",
      "Processing https://www.formula1.com/en/racing/2024/great-britain\n",
      "Processing https://www.formula1.com/en/racing/2024/united-arab-emirates\n",
      "Processing https://www.formula1.com/en/racing/2024/canada\n",
      "Processing https://www.formula1.com/en/racing/2024/monaco\n",
      "Processing https://www.formula1.com/en/racing/2024/saudi-arabia\n",
      "Processing https://www.formula1.com/en/racing/2024/brazil\n",
      "Processing https://www.formula1.com/en/racing/2024/miami\n",
      "Processing https://www.formula1.com/en/racing/2024/bahrain\n",
      "Processing https://www.formula1.com/en/racing/2024/japan\n",
      "Processing https://www.formula1.com/en/racing/2024/italy\n",
      "Processing https://www.formula1.com/en/racing/2024/united-states\n",
      "Processing https://www.formula1.com/en/racing/2024/spain\n",
      "Processing https://www.formula1.com/en/racing/2024/hungary\n",
      "Processing https://www.formula1.com/en/racing/2024/belgium\n",
      "Processing https://www.formula1.com/en/racing/2024/austria\n",
      "Data collection complete. Check the output directory: C:\\Users\\Saffat\\Downloads\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_unstructured_data(base_url, output_dir):\n",
    "    # Base URL for the 2024 season page\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the main page: {base_url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the main page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find links to all rounds\n",
    "    round_links = [\n",
    "        a['href'] for a in soup.find_all('a', href=True) if '/racing/2024/' in a['href'] and '/circuit' not in a['href']\n",
    "    ]\n",
    "    round_links = list(set([base_url.split('/en/racing/')[0] + link for link in round_links]))  # Make links absolute\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare JSON file to store unstructured data\n",
    "    json_file = os.path.join(output_dir, \"f1_2024_unstructured_data.json\")\n",
    "    unstructured_data = []\n",
    "\n",
    "    for round_link in round_links:\n",
    "        print(f\"Processing {round_link}\")\n",
    "\n",
    "        # Fetch round page\n",
    "        round_response = requests.get(round_link)\n",
    "        if round_response.status_code != 200:\n",
    "            print(f\"Failed to fetch round page: {round_link}\")\n",
    "            continue\n",
    "\n",
    "        round_soup = BeautifulSoup(round_response.content, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Fetch narrative content\n",
    "            narrative_content = round_soup.find_all('p')\n",
    "            narrative_text = \" \".join([p.text.strip() for p in narrative_content if p.text.strip()])\n",
    "\n",
    "            # Fetch image metadata (only those with 'carbon' in 'alt' attribute)\n",
    "            images = round_soup.find_all('img')\n",
    "            image_metadata = [{\"src\": img['src'], \"alt\": img.get('alt', 'No description')} for img in images \n",
    "                              if 'src' in img.attrs and 'carbon' in img.get('alt', '').lower()]\n",
    "\n",
    "            # Store data\n",
    "            unstructured_data.append({\n",
    "                \"round_link\": round_link,\n",
    "                \"narrative_text\": narrative_text,\n",
    "                \"image_metadata\": image_metadata\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {round_link}: {e}\")\n",
    "\n",
    "    # Save unstructured data to JSON file\n",
    "    with open(json_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(unstructured_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_URL = \"https://www.formula1.com/en/racing/2024\"\n",
    "    OUTPUT_DIR = \"C:\\\\Users\\\\Saffat\\\\Downloads\"\n",
    "\n",
    "    get_unstructured_data(BASE_URL, OUTPUT_DIR)\n",
    "    print(f\"Data collection complete. Check the output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Images 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.formula1.com/en/racing/2023/australia\n",
      "Processing https://www.formula1.com/en/racing/2023/italy\n",
      "Processing https://www.formula1.com/en/racing/2023/canada\n",
      "Processing https://www.formula1.com/en/racing/2023/azerbaijan\n",
      "Processing https://www.formula1.com/en/racing/2023/emiliaromagna\n",
      "Processing https://www.formula1.com/en/racing/2023/qatar\n",
      "Processing https://www.formula1.com/en/racing/2023/miami\n",
      "Processing https://www.formula1.com/en/racing/2023/japan\n",
      "Processing https://www.formula1.com/en/racing/2023/spain\n",
      "Processing https://www.formula1.com/en/racing/2023/bahrain\n",
      "Processing https://www.formula1.com/en/racing/2023/monaco\n",
      "Processing https://www.formula1.com/en/racing/2023/netherlands\n",
      "Processing https://www.formula1.com/en/racing/2023/hungary\n",
      "Processing https://www.formula1.com/en/racing/2023/mexico\n",
      "Processing https://www.formula1.com/en/racing/2023/las-vegas\n",
      "Processing https://www.formula1.com/en/racing/2023/singapore\n",
      "Processing https://www.formula1.com/en/racing/2023/united-states\n",
      "Processing https://www.formula1.com/en/racing/2023/brazil\n",
      "Processing https://www.formula1.com/en/racing/2023/saudi-arabia\n",
      "Processing https://www.formula1.com/en/racing/2023/austria\n",
      "Processing https://www.formula1.com/en/racing/2023/great-britain\n",
      "Processing https://www.formula1.com/en/racing/2023/belgium\n",
      "Processing https://www.formula1.com/en/racing/2023/pre-season-testing\n",
      "Processing https://www.formula1.com/en/racing/2023/united-arab-emirates\n",
      "Data collection complete. Check the output directory: C:\\Users\\Saffat\\Downloads\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_unstructured_data(base_url, output_dir):\n",
    "    # Base URL for the 2023 season page\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch the main page: {base_url}\")\n",
    "        return\n",
    "\n",
    "    # Parse the main page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find links to all rounds\n",
    "    round_links = [\n",
    "        a['href'] for a in soup.find_all('a', href=True) if '/racing/2023/' in a['href'] and '/circuit' not in a['href']\n",
    "    ]\n",
    "    round_links = list(set([base_url.split('/en/racing/')[0] + link for link in round_links]))  # Make links absolute\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare JSON file to store unstructured data\n",
    "    json_file = os.path.join(output_dir, \"f1_2023_unstructured_data.json\")\n",
    "    unstructured_data = []\n",
    "\n",
    "    for round_link in round_links:\n",
    "        print(f\"Processing {round_link}\")\n",
    "\n",
    "        # Fetch round page\n",
    "        round_response = requests.get(round_link)\n",
    "        if round_response.status_code != 200:\n",
    "            print(f\"Failed to fetch round page: {round_link}\")\n",
    "            continue\n",
    "\n",
    "        round_soup = BeautifulSoup(round_response.content, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            # Fetch narrative content\n",
    "            narrative_content = round_soup.find_all('p')\n",
    "            narrative_text = \" \".join([p.text.strip() for p in narrative_content if p.text.strip()])\n",
    "\n",
    "            # Fetch image metadata (only those with 'carbon' in 'alt' attribute)\n",
    "            images = round_soup.find_all('img')\n",
    "            image_metadata = [{\"src\": img['src'], \"alt\": img.get('alt', 'No description')} for img in images \n",
    "                              if 'src' in img.attrs and 'carbon' in img.get('alt', '').lower()]\n",
    "\n",
    "            # Store data\n",
    "            unstructured_data.append({\n",
    "                \"round_link\": round_link,\n",
    "                \"narrative_text\": narrative_text,\n",
    "                \"image_metadata\": image_metadata\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {round_link}: {e}\")\n",
    "\n",
    "    # Save unstructured data to JSON file\n",
    "    with open(json_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(unstructured_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    BASE_URL = \"https://www.formula1.com/en/racing/2023\"\n",
    "    OUTPUT_DIR = \"C:\\\\Users\\\\Saffat\\\\Downloads\"\n",
    "\n",
    "    get_unstructured_data(BASE_URL, OUTPUT_DIR)\n",
    "    print(f\"Data collection complete. Check the output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining both track image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to C:\\Users\\Saffat\\Downloads\\f1_2023_2024_combined_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to load and combine JSON data from two files with a year column\n",
    "def combine_json_files_with_year(file1, file2, output_file):\n",
    "    # Load data from the first JSON file (2023)\n",
    "    with open(file1, 'r', encoding='utf-8') as f1:\n",
    "        data1 = json.load(f1)\n",
    "    \n",
    "    # Add the year 2023 to each entry in data1\n",
    "    for entry in data1:\n",
    "        entry[\"year\"] = 2023\n",
    "\n",
    "    # Load data from the second JSON file (2024)\n",
    "    with open(file2, 'r', encoding='utf-8') as f2:\n",
    "        data2 = json.load(f2)\n",
    "\n",
    "    # Add the year 2024 to each entry in data2\n",
    "    for entry in data2:\n",
    "        entry[\"year\"] = 2024\n",
    "\n",
    "    # Combine both data\n",
    "    combined_data = data1 + data2\n",
    "\n",
    "    # Save combined data to the output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        json.dump(combined_data, output, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Combined data saved to {output_file}\")\n",
    "\n",
    "# Define file paths for the 2023 and 2024 JSON files (using local paths)\n",
    "file_2023 = r\"C:\\Users\\Saffat\\Downloads\\f1_2023_unstructured_data.json\"\n",
    "file_2024 = r\"C:\\Users\\Saffat\\Downloads\\f1_2024_unstructured_data.json\"\n",
    "output_file = r\"C:\\Users\\Saffat\\Downloads\\f1_2023_2024_combined_data.json\"\n",
    "\n",
    "# Combine the JSON files with the year column\n",
    "combine_json_files_with_year(file_2023, file_2024, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
